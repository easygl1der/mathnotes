\section{Probability and Distributions}

Section 3 introduces the fundamental definitions of probability functions on sets endowed with a $\sigma$-algebra structure. Key concepts discussed include the interchange of $\lim_{n \to \infty}$ and $P(\cdot)$, properties of disjoint unions ($\bigsqcup$), and DeMorgan's laws.

Section 4 elaborates on conditional probability and independence, presenting them as generalizations of probability theory with broad applicability. The conditional probability is defined computationally as $P(B|A)=P(AB)/P(A)$, while independence is defined as $P(B|A)=P(B)$, implying $P(AB)=P(A)P(B)$. The section concludes with a discussion of simulation methods for approximating probabilities in complex real-world scenarios, along with an introduction to error estimation, which is further explored in Chapter 4 in the context of \textbf{confidence intervals}.

Section 5 introduces the concept of random variables as functions mapping from a sample space (equipped with a sigma algebra) to $\mathbb{R}$. It distinguishes between the probability mass function (pmf) for discrete distributions and the probability density function (pdf) for continuous distributions, denoted by $p_{X}(x)$. The cumulative distribution function (cdf) is defined as $F_{X}(x)=P_{X}((-\infty,x])=P(\{ c\in \mathcal{C}:X(c)\le x \})$, where $\mathcal{C}$ represents the sample space. The notion of \textbf{equality in distribution} is defined as $X\overset{D}{=}Y$ if and only if $F_{X}=F_{Y}$. However, it is noted that $X\overset{D}{=}Y$ does not necessarily imply $X=Y$ (e.g., $X\sim U(0,1),Y=1-X$). The section also outlines the properties of cdfs, including being nondecreasing, having a lower limit of 0, an upper limit of 1, and being right continuous. Additionally, it establishes that $P_{X}((a,b])=F_{X}(b)-F_{X}(a)$.

Section 6 focuses on discrete random variables, whose sample space is at most countable, and explores transformations between random variables, i.e., $Y=g(X)$. In the case where $g$ is one-to-one, the pmf of $Y$ is given by $p_{Y}=p_{X}(g^{-1}(y))$ ($X$ and $Y$ are discrete). When $g$ is piecewise one-to-one, the definition of $p_{Y}$ requires a case-by-case analysis.

Section 7 introduces continuous random variables, where the cdf is given by $F_{X}(x)=\int_{-\infty}^{x} f_{X}(t) \, dt$. The quantile of order $p$ of the distribution of $X$ is defined as a value $\xi_{p}$ such that $P(X<\xi_{p})\leq p$ and $P(X\leq \xi_{p})\geq p$, also known as the ($100p$) th \textbf{percentile} of $X$. It is noted that the quantile may not be unique and that $\xi_{p}\in F^{-1}_{X}(p)$. For transformations, the pdf of $Y$ is given by $f_{Y}(y)=f_{X}(g^{-1}(y))\left\lvert \frac{dx}{dy} \right\rvert$ for $y$ in $\mathcal{S}_{Y}=g(\mathcal{S}_{X})$, the support of $Y$. For continuous random vectors, $\frac{dx}{dy}$ refers to the Jacobian of the transformation.

Section 8 discusses the \textbf{expectation} $E(X)=\int_{-\infty}^{\infty} xf(x) \, dx$, and $E(g(X))=\int_{-\infty}^{\infty} g(x)f_{X}(x) \, dx$.

Section 9 covers specific expectations, including the mean $\mu=E(X)$, the variance $\sigma^{2}=E((X-E(X))^{2})$, and the \textbf{moment generating function} (mgf) $M(t)\coloneqq E(e^{tX})$ (if it exists for $t\in(-h,h)$). The derivatives of $M(t)$ are explored, noting that $M'(0)=\mu$, $M''(0)=E(X^{2})$, and $M^{(m)}(0)=E(X^{m})$ is called the $m$ th moment of $X$.

Section 10 presents important inequalities: the existence of $E[\lvert X \rvert ^{m}]$ implies the existence of $E[\lvert X \rvert ^{k}]$ for $k\leq m$. Markov's Inequality is stated as $P(u(X)\geq c)\leq\frac{E(u(X))}{c}$ for nonnegative $u$ and positive $c$. Chebyshev's Inequality is given as: for $X$ with $\sigma^{2}<\infty$, for $k>0$ we have $P(\lvert X-\mu \rvert\geq k\sigma)\leq\frac{1}{k^{2}}$. Jensen's Inequality states that for a convex function $\phi$ ($\phi''\geq0$), we have $\phi ( E (X))\le E(\phi(X))$.

Chapter 2 delves into multivariate distributions. Section 1 introduces the basic definitions and calculation techniques for two random variables. Section 2 presents transformations and the \textbf{moment generating function (mgf) technique}, which is effective for linear functions of random variables. If $Y=g(X_{1},X_{2})$ then $E(Y)=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x_{1},x_{2})f_{X_{1},X_{2}}(x_{1},x_{2}) \, dx_{1} \, dx_{2}$.

Section 3 introduces conditional distributions and expectations. The conditional pmf is given by $p_{X_{1}|X_{2}}(x_{1}|x_{2})=\frac{p_{X_{1}, X_{2}}(x_{1}, x_{2})}{p_{X_{2}}(x_{2})}, x_{1}\in \mathcal{S}_{X_{1}}$, and the conditional pdf is $f_{X_{1}|X_{2}}(x_{1}|x_{2})=\frac{f_{X_{1},X_{2}}(x_{1},x_{2})}{f_{X_{2}}(x_{2})},f_{X_{2}}(x_{2})>0$. The probability $P(X_{2}\in(a,b)|X_{1}=x_{1})=\int_{a}^{b} f_{2|1}(x_{2}|x_{1}) \, dx_{2}$ and $P(X_{1}\in(c,d)|X_{2}=x_{2})=\int_{c}^{d} f_{1|2}(x_{1}|x_{2}) \, dx_{1}$. The \textbf{conditional expectation} of $u(X_{2})$ ($u$ is a function of $X_{2}$) is $E[u(X_{2})|x_{1}]=\int_{-\infty}^{\infty} u(x_{2})f_{2|1}(x_{2}|x_{1}) \, dx_{2}$. The \textbf{conditional variance} is $\mathrm{Var}(X_{2}|x_{1})=E(X_{2}^{2}|x_{1})-[E(X_{2}|x_{1})]^{2}$. Conditional expectation satisfies $E[E(X_2|X_1)]=E(X_2)$, $Var[E(X_2|X_1)]\leq Var(X_2)$ which can serve as an alternative definition of conditional expectation.

Section 4 introduces the concept of independent random variables. If $X_1,X_2$ have the joint pdf $f(x_1,x_2)$, then $X_1,X_2$ are independent if and only if $f(x_1,x_2)$ can be written as $f(x_1,x_2)=g(x_1)h(x_2)$, where $g$ and $h$ are nonnegative functions. For independent $X_1,X_2$, we have $E[u(X_1)v(X_2)]=E[u(X_1)]E[u(X_2)]$. If $M(t_1,t_2)=E(e^{t_1X_1+t_2X_2})$ is the joint mgf, then $X_1,X_2$ are independent if and only if $M(t_1,t_2)=M(t_1,0)M(0,t_2)$.

Section 5 introduces the correlation coefficient $\rho=\frac{cov(X,Y)}{\sigma_1\sigma_2}$, where the covariance $cov(X,Y)=E[(X-\mu_1)(Y-\mu_2)]$. Independence implies that random variables are uncorrelated.

Section 6 extends the discussion to several random variables. $X_1,X_2,\dots,X_{n}$ are said to be mutually independent if and only if $f(x_1,\dots,x_{n})=f_1(x_1)\dots f_{n}(x_{n})$. Consequently, $E[u_1(X_1)\dots u_{n}(X_{n})]=E[u_1(X_1)]\dots E[u_{n}(X_{n})]$. Let $\mathbf{X}=(X_1,\dots X_{n})'$ be a $n$-dimensional random vector, $E(\mathbf{X})=(E(X_1),\dots,E(X_{n}))'$. If $\mathbf{W}$ is an $m\times n$ matrix of random variables then the \textbf{mean} of $\mathbf{W}$ is $E[\mathbf{W}]=[E(W_{ij})]$, the \textbf{variance-covariance matrix} is $Cov(\mathbf{X})=E[(\mathbf{X}-\boldsymbol{\mu})(\mathbf{X}-\boldsymbol{\mu})']=[\sigma _{ij}]$ ($\sigma _{ij}=Cov(X_i,X_j)$), and $Cov(\mathbf{X})=E[\mathbf{X}\mathbf{X}']-\boldsymbol{\mu}\boldsymbol{\mu}'$ and $Cov(\mathbf{AX})=\mathbf{A}Cov(\mathbf{X})\mathbf{A}'$. All variance-covariance matrices are positive semi-definite, because $\mathbf{a}'Cov (\mathbf{X})\mathbf{a}=Var (\mathbf{a}'\mathbf{X})=Var (Y)\geq0$ where $Y=\mathbf{a}'\mathbf{X}$ is a random variable.

Section 7 discusses transformations for several random variables. Section 8 addresses the linear combinations of random variables. If $X_1,\dots,X_n$ are iid, then these random variables constitute a \textbf{random sample} of size $n$ from that common distribution. The \textbf{sample variance} is defined by $S^{2}=(n-1)^{-1}\sum_{i=1}^{n}(X_i-\overline{X})^{2}=(n-1)^{-1}\left( \sum_{i=1}^{n}X_i^{2} -n\overline{X}^{2}\right)$.
